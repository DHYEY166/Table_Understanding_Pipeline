{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from pandas) (2.1.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conda in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (24.7.1)\n",
      "Requirement already satisfied: archspec>=0.2.3 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (0.2.3)\n",
      "Requirement already satisfied: boltons>=23.0.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (24.0.0)\n",
      "Requirement already satisfied: charset-normalizer in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (3.3.2)\n",
      "Requirement already satisfied: conda-libmamba-solver>=23.11.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (24.1.0)\n",
      "Requirement already satisfied: conda-package-handling>=2.2.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (2.2.0)\n",
      "Requirement already satisfied: distro>=1.5.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (1.9.0)\n",
      "Requirement already satisfied: frozendict>=2.4.2 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (2.4.4)\n",
      "Requirement already satisfied: jsonpatch>=1.32 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (1.33)\n",
      "Requirement already satisfied: menuinst>=2 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (2.0.2)\n",
      "Requirement already satisfied: packaging>=23.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (24.0)\n",
      "Requirement already satisfied: platformdirs>=3.10.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (4.2.0)\n",
      "Requirement already satisfied: pluggy>=1.0.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (1.4.0)\n",
      "Requirement already satisfied: pycosat>=0.6.3 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (0.6.6)\n",
      "Requirement already satisfied: requests<3,>=2.28.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (2.31.0)\n",
      "Requirement already satisfied: ruamel-yaml<0.19,>=0.11.14 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (0.18.6)\n",
      "Requirement already satisfied: setuptools>=60.0.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (69.5.1)\n",
      "Requirement already satisfied: tqdm>=4 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (4.66.2)\n",
      "Requirement already satisfied: truststore>=0.8.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (0.8.0)\n",
      "Requirement already satisfied: zstandard>=0.15 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda) (0.22.0)\n",
      "Requirement already satisfied: libmambapy>=1.5.6 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda-libmamba-solver>=23.11.0->conda) (1.5.8)\n",
      "Requirement already satisfied: conda-package-streaming>=0.9.0 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from conda-package-handling>=2.2.0->conda) (0.9.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from jsonpatch>=1.32->conda) (2.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.28.0->conda) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.28.0->conda) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from requests<3,>=2.28.0->conda) (2024.8.30)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /Users/dhyeydesai/miniforge3/lib/python3.10/site-packages (from ruamel-yaml<0.19,>=0.11.14->conda) (0.2.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install conda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate table-understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# packages in environment at /Users/dhyeydesai/miniforge3:\n",
      "#\n",
      "# Name                    Version                   Build  Channel\n",
      "openpyxl                  3.1.5           py310h0e452f4_1    conda-forge\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda list openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "# All requested packages already installed.\n",
      "\n",
      "\n",
      "CondaError: Run 'conda init' before 'conda activate'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!conda install --yes --prefix {sys.prefix} openpyxl\n",
    "\n",
    "!conda activate table-understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/99p9qj_s35zd5p2_p1g6xh4w0000gn/T/ipykernel_97652/839853245.py:15: DeprecationWarning: conda.cli.python_api is deprecated and will be removed in 24.9. Use `conda.testing.conda_cli` instead.\n",
      "  from conda.cli.python_api import run_command, Commands\n"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import io\n",
    "import subprocess\n",
    "import yaml\n",
    "from typing import List, Tuple, Optional\n",
    "import zipfile\n",
    "import sys\n",
    "import platform\n",
    "import traceback\n",
    "import pandas as pd\n",
    "from conda.cli.python_api import run_command, Commands\n",
    "from conda.exceptions import EnvironmentLocationNotFound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(url: str, filename: str):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_info(json_file_path: str) -> Tuple[float, float, List[str], List[List[str]]]:\n",
    "    try:\n",
    "        with open(json_file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        study = data['study'][0]\n",
    "\n",
    "        lat = study['site'][0]['geo']['geometry']['coordinates'][1]\n",
    "        lon = study['site'][0]['geo']['geometry']['coordinates'][0]\n",
    "\n",
    "        file_urls = []\n",
    "        variables_list = []\n",
    "\n",
    "        for site in study['site']:\n",
    "            for paleo_data in site['paleoData']:\n",
    "                # Access the first element within 'dataFile'\n",
    "                data_file = paleo_data['dataFile'][0]\n",
    "\n",
    "                file_urls.append(data_file['fileUrl'])\n",
    "\n",
    "                # Access 'variables' within the first element of 'dataFile'\n",
    "                variables_list.append([v.get('variableName', None) for v in data_file.get('variables', [])])\n",
    "\n",
    "        return lat, lon, file_urls, variables_list\n",
    "\n",
    "    except (FileNotFoundError, json.JSONDecodeError, KeyError) as e:\n",
    "        print(f\"Error loading dataset info: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dust-coreb.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/99p9qj_s35zd5p2_p1g6xh4w0000gn/T/ipykernel_97652/3268293368.py:31: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pandas.read_csv(io.StringIO('\\n'.join(data_lines)), delim_whitespace=True, comment='#', header=0, names=column_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed data from https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dust-coreb.txt. Shape: (4341, 4)\n",
      "   age_calBP  dustfrac   sedrate      dens\n",
      "0    4483.51  0.168678  0.055556  0.195990\n",
      "1    4482.57  0.156434  0.055556  0.200526\n",
      "2    4481.46  0.164476  0.055556  0.205818\n",
      "3    4480.36  0.225408  0.055556  0.211110\n",
      "4    4479.26  0.131894  0.055556  0.216402\n",
      "Columns: ['age_calBP', 'dustfrac', 'sedrate', 'dens']\n",
      "\n",
      "\n",
      "Processing file: https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dmar-ens.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/99p9qj_s35zd5p2_p1g6xh4w0000gn/T/ipykernel_97652/3268293368.py:31: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pandas.read_csv(io.StringIO('\\n'.join(data_lines)), delim_whitespace=True, comment='#', header=0, names=column_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed data from https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dmar-ens.txt. Shape: (199, 6)\n",
      "   age_calBP     DMAR   CI_0.84  CI_0.16  CI_0.25  CI_0.975\n",
      "0     -46.55  40.1265   55.9888  24.8088  -2.3479   72.0817\n",
      "1     -23.64  41.6070   59.0063  23.8030  15.7566   85.1572\n",
      "2      -0.74  74.0322  100.2443  46.9366  32.8553  135.4476\n",
      "3      22.17  50.7744   77.1108  27.8263  17.7682  103.2618\n",
      "4      45.07  21.4330   29.8379  11.7334   7.7101   42.9134\n",
      "Columns: ['age_calBP', 'DMAR', 'CI_0.84', 'CI_0.16', 'CI_0.25', 'CI_0.975']\n",
      "\n",
      "\n",
      "Processing file: https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dmar.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/99p9qj_s35zd5p2_p1g6xh4w0000gn/T/ipykernel_97652/3268293368.py:31: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pandas.read_csv(io.StringIO('\\n'.join(data_lines)), delim_whitespace=True, comment='#', header=0, names=column_names)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully processed data from https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dmar.txt. Shape: (2471, 2)\n",
      "   age_calBP     DMAR\n",
      "0     -60.12  58.8432\n",
      "1     -58.75  71.7891\n",
      "2     -57.38  44.6904\n",
      "3     -56.01  36.6339\n",
      "4     -54.65  38.6674\n",
      "Columns: ['age_calBP', 'DMAR']\n",
      "\n",
      "\n",
      "Processing file: https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dust-corea.txt\n",
      "Successfully processed data from https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dust-corea.txt. Shape: (2769, 4)\n",
      "   age_calBP  dustfrac   sedrate     dens\n",
      "0    2722.83  0.337020  0.049261  0.24186\n",
      "1    2722.12  0.491964  0.049261  0.24114\n",
      "2    2721.21  0.272375  0.049261  0.24424\n",
      "3    2720.30  0.262410  0.049261  0.24734\n",
      "4    2719.48  0.272334  0.049261  0.25009\n",
      "Columns: ['age_calBP', 'dustfrac', 'sedrate', 'dens']\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8z/99p9qj_s35zd5p2_p1g6xh4w0000gn/T/ipykernel_97652/3268293368.py:31: FutureWarning: The 'delim_whitespace' keyword in pd.read_csv is deprecated and will be removed in a future version. Use ``sep='\\s+'`` instead\n",
      "  df = pandas.read_csv(io.StringIO('\\n'.join(data_lines)), delim_whitespace=True, comment='#', header=0, names=column_names)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import io\n",
    "\n",
    "def download_and_process_data(file_url: str, variables: Optional[list[str]] = None) -> pandas.DataFrame:\n",
    "    try:\n",
    "        response = requests.get(file_url)\n",
    "        response.raise_for_status()\n",
    "        content = response.text\n",
    "\n",
    "        # Find the start of the data section\n",
    "        data_start = content.find('Data:')\n",
    "        if data_start == -1:\n",
    "            raise ValueError(\"Data section not found in the file\")\n",
    "\n",
    "        # Extract the data section\n",
    "        data_section = content[data_start + 5:].strip()  # Skip 'Data:' and any leading/trailing whitespace\n",
    "\n",
    "        # Split the data section into lines\n",
    "        data_lines = data_section.splitlines()\n",
    "\n",
    "        # Find the first non-comment line (assuming it contains the header)\n",
    "        header_line = next((line for line in data_lines if not line.startswith('#')), None)\n",
    "\n",
    "        if header_line:\n",
    "            # Remove comment character if present and split into column names\n",
    "            column_names = [col.replace('#', '').strip() for col in header_line.split()]\n",
    "            # Filter out empty column names\n",
    "            column_names = [col for col in column_names if col]\n",
    "\n",
    "            # Read the data, skipping the first row (header) if it's duplicated\n",
    "            df = pandas.read_csv(io.StringIO('\\n'.join(data_lines)), delim_whitespace=True, comment='#', header=0, names=column_names)\n",
    "            if df.iloc[0].equals(pandas.Series(column_names)):  # Check if first row is the header\n",
    "                df = df.iloc[1:].copy()  # Skip the first row if it's a duplicate header\n",
    "        else:\n",
    "            # If no header line is found, read the data without column names\n",
    "            df = pandas.read_csv(io.StringIO(data_section), delim_whitespace=True, comment='#', header=None)\n",
    "\n",
    "        if df.empty:\n",
    "            raise ValueError(\"No data was parsed from the file\")\n",
    "\n",
    "        # If variables are provided and match the number of columns, use them\n",
    "        if variables and len(variables) == df.shape[1]:\n",
    "            df.columns = variables\n",
    "\n",
    "        print(f\"Successfully processed data from {file_url}. Shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading data from {file_url}: {e}\")\n",
    "    except ValueError as e:\n",
    "        print(f\"Error processing data from {file_url}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing data from {file_url}: {e}\")\n",
    "\n",
    "    return pandas.DataFrame()\n",
    "\n",
    "# Example usage:\n",
    "urls = [\n",
    "    \"https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dust-coreb.txt\",\n",
    "    \"https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dmar-ens.txt\",\n",
    "    \"https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dmar.txt\",\n",
    "    \"https://www.ncei.noaa.gov/pub/data/paleo/paleolimnology/northamerica/usa/colorado/blue2019dust-corea.txt\"\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    print(f\"Processing file: {url}\")\n",
    "    # You can provide variables if you know them, or leave it as None\n",
    "    variables = None  # or [list of variable names if known]\n",
    "    df = download_and_process_data(url, variables)\n",
    "    if not df.empty:\n",
    "        print(df.head())\n",
    "        print(f\"Columns: {df.columns.tolist()}\")\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment created successfully.\n",
      "Packages installed successfully.\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "\n",
    "def setup_conda_environment():\n",
    "    env_name = \"table-understanding\"\n",
    "\n",
    "    # Create the environment\n",
    "    create_env_cmd = [\"conda\", \"create\", \"-n\", env_name, \"python=3.10\", \"-y\"]\n",
    "    result = subprocess.run(create_env_cmd, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"Environment created successfully.\")\n",
    "    else:\n",
    "        print(f\"Error creating environment: {result.stderr}\")\n",
    "        return\n",
    "\n",
    "    # Install packages\n",
    "    install_cmd = [\"conda\", \"run\", \"-n\", env_name, \"pip\", \"install\", \"pandas\", \"requests\", \"pyyaml\"]\n",
    "    result = subprocess.run(install_cmd, capture_output=True, text=True)\n",
    "\n",
    "    if result.returncode == 0:\n",
    "        print(\"Packages installed successfully.\")\n",
    "    else:\n",
    "        print(f\"Error installing packages: {result.stderr}\")\n",
    "\n",
    "setup_conda_environment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_in_conda_env(env_name: str, command: str):\n",
    "    try:\n",
    "        result = run_command(Commands.RUN, [\"-n\", env_name] + command.split())\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error running command in conda environment: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_isi_table_understanding(env_name: str, skip_psl_download: bool = False):\n",
    "    try:\n",
    "        if os.path.exists(\"isi-table-understanding\"):\n",
    "            print(\"isi-table-understanding directory already exists. Updating...\")\n",
    "            os.chdir(\"isi-table-understanding\")\n",
    "            subprocess.run([\"git\", \"pull\"], check=True)\n",
    "        else:\n",
    "            subprocess.run([\"git\", \"clone\", \"-b\", \"impl\", \"https://github.com/usc-isi-i2/isi-table-understanding.git\"], check=True)\n",
    "            os.chdir(\"isi-table-understanding\")\n",
    "        if os.path.exists(\"requirements.txt\"):\n",
    "            run_in_conda_env(env_name, \"pip install -r requirements.txt\")\n",
    "        else:\n",
    "            print(\"requirements.txt not found. Skipping package installation.\")\n",
    "        if not skip_psl_download:\n",
    "            os.makedirs(\"data\", exist_ok=True)\n",
    "            psl_url = input(\"Please enter the URL for the PSL files zip (or press Enter to skip): \")\n",
    "            if psl_url:\n",
    "                try:\n",
    "                    download_file(psl_url, \"data/psl_files.zip\")\n",
    "                    with zipfile.ZipFile(\"data/psl_files.zip\", 'r') as zip_ref:\n",
    "                        zip_ref.extractall(\"data/\")\n",
    "                    print(\"PSL files downloaded and extracted successfully.\")\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    print(f\"Failed to download PSL files: {e}\")\n",
    "                    print(\"Continuing without PSL files. Some functionality may be limited.\")\n",
    "            else:\n",
    "                print(\"Skipping PSL files download.\")\n",
    "        else:\n",
    "            print(\"Skipping PSL files download.\")\n",
    "        if not os.path.exists(\"InferSent\"):\n",
    "            subprocess.run([\"git\", \"clone\", \"https://github.com/facebookresearch/InferSent.git\"], check=True)\n",
    "        os.chdir(\"InferSent\")\n",
    "        os.makedirs(\"GloVe\", exist_ok=True)\n",
    "        glove_url = \"http://nlp.stanford.edu/data/glove.840B.300d.zip\"\n",
    "        download_file(glove_url, \"GloVe/glove.840B.300d.zip\")\n",
    "        with zipfile.ZipFile(\"GloVe/glove.840B.300d.zip\", 'r') as zip_ref:\n",
    "            zip_ref.extractall(\"GloVe/\")\n",
    "        os.makedirs(\"encoder\", exist_ok=True)\n",
    "        download_file(\"https://dl.fbaipublicfiles.com/infersent/infersent1.pkl\", \"encoder/infersent1.pkl\")\n",
    "        download_file(\"https://dl.fbaipublicfiles.com/infersent/infersent2.pkl\", \"encoder/infersent2.pkl\")\n",
    "        os.chdir(\"..\")\n",
    "        psl_config = {\n",
    "            \"psl\": {\n",
    "                \"model_path\": os.path.abspath(\"path/to/pretrained/psl/model\"),\n",
    "                \"infersent_path\": os.path.abspath(\"InferSent\"),\n",
    "                \"glove_path\": os.path.abspath(\"InferSent/GloVe/glove.840B.300d.txt\")\n",
    "            }\n",
    "        }\n",
    "        os.makedirs(\"cfg\", exist_ok=True)\n",
    "        with open(\"cfg/psl_config.yaml\", \"w\") as f:\n",
    "            yaml.dump(psl_config, f)\n",
    "        print(\"ISI Table Understanding setup completed successfully.\")\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error in setup: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error in setup: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_table_understanding(env_name: str, input_file: str, output_dir: str):\n",
    "    try:\n",
    "        config_file = \"cfg/psl_config.yaml\"\n",
    "        files_config = f\"- {input_file}\"\n",
    "\n",
    "        with open(\"cfg/files.yaml\", \"w\") as f:\n",
    "            f.write(files_config)\n",
    "\n",
    "        result = run_in_conda_env(env_name, f\"python main.py --config {config_file} --files cfg/files.yaml --output {output_dir}\")\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        print(f\"Error running table understanding: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(skip_psl_download: bool = False):\n",
    "    try:\n",
    "        json_file_path = 'data.json'\n",
    "        output_dir = os.path.abspath('./output')\n",
    "\n",
    "        print(f\"Current working directory: {os.getcwd()}\")\n",
    "        print(f\"Attempting to create output directory: {output_dir}\")\n",
    "\n",
    "        # Ensure output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        print(f\"Output directory created/confirmed: {output_dir}\")\n",
    "        print(f\"Output directory exists: {os.path.exists(output_dir)}\")\n",
    "        print(f\"Output directory is writable: {os.access(output_dir, os.W_OK)}\")\n",
    "\n",
    "        lat, lon, file_urls, variables_list = load_dataset_info(json_file_path)\n",
    "\n",
    "        print(f\"Latitude: {lat}, Longitude: {lon}\")\n",
    "\n",
    "        env_name = setup_conda_environment()\n",
    "        setup_isi_table_understanding(env_name, skip_psl_download)\n",
    "\n",
    "        results = []\n",
    "        for file_url, variables in zip(file_urls, variables_list):\n",
    "            print(f\"Processing file: {file_url}\")\n",
    "            print(f\"Variables: {variables}\")\n",
    "\n",
    "            # If variables are all None, set it to None to let download_and_process_data handle it\n",
    "            if all(var is None for var in variables):\n",
    "                variables = None\n",
    "\n",
    "            df = download_and_process_data(file_url, variables)\n",
    "            if df.empty:\n",
    "                print(f\"Skipping empty dataframe for {file_url}\")\n",
    "                continue\n",
    "\n",
    "            print(\"DataFrame head:\")\n",
    "            print(df.head())\n",
    "\n",
    "            csv_file = os.path.join(output_dir, f\"{os.path.basename(file_url)}.csv\")\n",
    "            print(f\"Attempting to save file: {csv_file}\")\n",
    "            try:\n",
    "                df.to_csv(csv_file, index=False)\n",
    "                print(f\"Saved data to {csv_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving data to {csv_file}: {e}\")\n",
    "                print(f\"File path exists: {os.path.exists(os.path.dirname(csv_file))}\")\n",
    "                print(f\"File path is writable: {os.access(os.path.dirname(csv_file), os.W_OK)}\")\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                table_understanding_result = run_table_understanding(env_name, csv_file, output_dir)\n",
    "\n",
    "                # Load table understanding output\n",
    "                with open(os.path.join(output_dir, \"table-understanding-results.json\"), \"r\") as f:\n",
    "                    table_blocks = json.load(f)\n",
    "\n",
    "                # Evaluate block identification\n",
    "                for block in table_blocks:\n",
    "                    block_columns = block[\"col_headers\"]\n",
    "                    block_data = block[\"cells\"]\n",
    "\n",
    "                    # Check if all columns in a block correspond to a single variable\n",
    "                    is_correct = any(all(col in var for col in block_columns) for var in variables)\n",
    "\n",
    "                    print(f\"Block: {block_columns}\")\n",
    "                    print(f\"Data Series:\")\n",
    "                    for row in block_data:\n",
    "                        print(row)\n",
    "                    print(f\"Correctly identified: {is_correct}\\n\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error running table understanding or evaluating results: {e}\")\n",
    "                table_understanding_result = None\n",
    "\n",
    "            results.append({\n",
    "                'file_url': file_url,\n",
    "                'dataframe': df,\n",
    "                'table_understanding_result': table_understanding_result\n",
    "            })\n",
    "\n",
    "            print(\"\\n\")\n",
    "\n",
    "        print(\"Processing complete. Check the output directory for results.\")\n",
    "        return results\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred in main: {e}\")\n",
    "        traceback.print_exc()\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /Users/dhyeydesai/Downloads/unique\n",
      "Attempting to create output directory: /Users/dhyeydesai/Downloads/unique/output\n",
      "Output directory created/confirmed: /Users/dhyeydesai/Downloads/unique/output\n",
      "Output directory exists: True\n",
      "Output directory is writable: True\n",
      "Latitude: 119.078, Longitude: 1.4033\n",
      "Environment created successfully.\n",
      "Packages installed successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'isi-table-understanding'...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "requirements.txt not found. Skipping package installation.\n",
      "Skipping PSL files download.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'InferSent'...\n"
     ]
    }
   ],
   "source": [
    "# Run the main function and display results\n",
    "results = main(skip_psl_download=True)\n",
    "if results:\n",
    "    for result in results:\n",
    "        print(f\"File: {result['file_url']}\")\n",
    "        print(\"DataFrame head:\")\n",
    "        display(result['dataframe'].head())\n",
    "        print(\"Table Understanding Result:\")\n",
    "        print(result['table_understanding_result'])\n",
    "        print(\"\\n\")\n",
    "else:\n",
    "    print(\"No results were returned. Check the error messages above for more information.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "1af9a4da2baec28e0252454627ef4d0a9cd8dcc9ab147c9111d2056501a8f3e2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
